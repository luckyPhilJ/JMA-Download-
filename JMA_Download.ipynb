{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "気象庁から過去の気象データを CSV 形式でダウンロードする。\n",
    "API が提供されていないので、ウェブページを参考にスクリプトを作成した。\n",
    "\n",
    "http://www.data.jma.go.jp/gmd/risk/obsdl/index.php\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import pandas as pd\n",
    "import codecs as cd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    return urllib.parse.urlencode(data).encode(encoding='ascii')\n",
    "\n",
    "def get_phpsessid():\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/index.php\"\n",
    "    xml = urllib.request.urlopen(URL).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "    return tree.cssselect(\"input#sid\")[0].value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns obsevation stations in prefectures\n",
    "\n",
    "def get_station(pd=0):\n",
    "    assert type(pd) is int and pd > 0\n",
    "    \n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/station\"\n",
    "    data = encode_data({\"pd\": \"%02d\" % pd})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def kansoku_items(bits):\n",
    "        return dict(rain=(bits[0] == \"1\"),\n",
    "                    wind=(bits[1] == \"1\"),\n",
    "                    temp=(bits[2] == \"1\"),\n",
    "                    sun =(bits[3] == \"1\"),\n",
    "                    snow=(bits[4] == \"1\"))\n",
    "\n",
    "    def parse_station(dom):\n",
    "        stitle = dom.get(\"title\").replace(\"：\", \":\")\n",
    "        title = dict(filter(lambda y: len(y) == 2,\n",
    "                            map(lambda x: x.split(\":\"), stitle.split(\"\\n\"))))\n",
    "                                \n",
    "        name    = title[\"地点名\"]\n",
    "        stid    = dom.cssselect(\"input[name=stid]\")[0].value\n",
    "        stname  = dom.cssselect(\"input[name=stname]\")[0].value\n",
    "        kansoku = kansoku_items(dom.cssselect(\"input[name=kansoku]\")[0].value)\n",
    "        assert name == stname\n",
    "        return (stname, dict(id=stid, flags=kansoku))\n",
    "    \n",
    "    \n",
    "    stations = dict(map(parse_station, tree.cssselect(\"div.station\")))\n",
    "    \n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precture Dictionary\n",
    "# order 0: id -> name, 1: name -> id\n",
    "\n",
    "def get_prefs(order, pd=0):\n",
    "        \n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/station\"\n",
    "    data = encode_data({\"pd\": \"%02d\" % pd})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def parse_prefs(dom):\n",
    "        name = dom.text\n",
    "        prid = int(dom.cssselect(\"input[name=prid]\")[0].value)\n",
    "        if order == 0:\n",
    "            return (prid, name)\n",
    "        else:\n",
    "            return (name, prid)\n",
    "    \n",
    "    stations = dict(map(parse_prefs, tree.cssselect(\"div.prefecture\")))\n",
    "        \n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_aggrgPeriods():\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/element\"\n",
    "    xml = urllib.request.urlopen(URL).read().decode(\"utf-8\")  # HTTP GET\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def parse_periods(dom):\n",
    "        if dom.find(\"label\") is not None:\n",
    "            val = dom.find(\"label/input\").attrib[\"value\"]\n",
    "            key = dom.find(\"label/span\").text\n",
    "            rng = None\n",
    "        else:\n",
    "            val = dom.find(\"input\").attrib[\"value\"]\n",
    "            key = dom.find(\"span/label\").text\n",
    "            rng = list(map(lambda x: int(x.get(\"value\")),\n",
    "                           dom.find(\"span/select\").getchildren()))\n",
    "        return (key, (val, rng))\n",
    "\n",
    "    perdoms = tree.cssselect(\"#aggrgPeriod\")[0].find(\"div/div\").getchildren()\n",
    "    periods = dict(map(parse_periods, perdoms))\n",
    "    return periods\n",
    "\n",
    "def get_elements(aggrgPeriods=9, isTypeNumber=1):\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/element\"\n",
    "    data = encode_data({\"aggrgPeriod\": aggrgPeriods,\n",
    "                        \"isTypeNumber\": isTypeNumber})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    open(\"tmp.html\", \"w\").write(xml)\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    boxes = tree.cssselect(\"input[type=checkbox]\")\n",
    "    options, items = boxes[0:4], boxes[4:]\n",
    "\n",
    "    def parse_items(dom):\n",
    "        if \"disabled\" in dom.attrib: return None\n",
    "        if dom.name == \"kijiFlag\": return None\n",
    "        name     = dom.attrib[\"id\"]\n",
    "        value    = dom.attrib[\"value\"]\n",
    "        options  = None\n",
    "        select = dom.getnext().find(\"select\")\n",
    "        if select is not None:\n",
    "            options = list(map(lambda x: int(x.get(\"value\")),\n",
    "                               select.getchildren()))\n",
    "        return (name, (value, options))\n",
    "    \n",
    "    items = dict(filter(lambda x: x, map(parse_items, items)))\n",
    "    return items\n",
    "\n",
    "\n",
    "def download_hourly_csv(phpsessid, station, element, begin_date, end_date):\n",
    "    params = {\n",
    "        \"PHPSESSID\": phpsessid,\n",
    "        # 共通フラグ\n",
    "        \"rmkFlag\": 1,        # 利用上注意が必要なデータを格納する\n",
    "        \"disconnectFlag\": 1, # 観測環境の変化にかかわらずデータを格納する\n",
    "        \"csvFlag\": 1,        # すべて数値で格納する\n",
    "        \"ymdLiteral\": 1,     # 日付は日付リテラルで格納する\n",
    "        \"youbiFlag\": 0,      # 日付に曜日を表示する\n",
    "        \"kijiFlag\": 0,       # 最高・最低（最大・最小）値の発生時刻を表示\n",
    "        # 時別値データ選択\n",
    "        \"aggrgPeriod\": 9,    # 時別値\n",
    "        \"stationNumList\": '[\"%s\"]' % station,      # 観測地点IDのリスト\n",
    "        \"elementNumList\": '[[\"%s\",\"\"]]' % element, # 項目IDのリスト\n",
    "        \"ymdList\": '[\"%d\", \"%d\", \"%d\", \"%d\", \"%d\", \"%d\"]' % (\n",
    "            begin_date.year,  end_date.year,\n",
    "            begin_date.month, end_date.month,\n",
    "            begin_date.day,   end_date.day),       # 取得する期間\n",
    "        \"jikantaiFlag\": 0,        # 特定の時間帯のみ表示する\n",
    "        \"jikantaiList\": '[1,24]', # デフォルトは全部\n",
    "        \"interAnnualFlag\": 1,     # 連続した期間で表示する\n",
    "        # 以下、意味の分からないフラグ類\n",
    "        \"optionNumList\": [],\n",
    "        \"downloadFlag\": \"true\",   # CSV としてダウンロードする？\n",
    "        \"huukouFlag\": 0,\n",
    "    }\n",
    "\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/show/table\"\n",
    "    data = encode_data(params)\n",
    "    csv_data = urllib.request.urlopen(URL, data=data).read().decode(\"shift-jis\")\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of start and end-dates\n",
    "# As max size of data is about 1 years worth of hourly data it divides period into yearly segments\n",
    "\n",
    "def get_dates(start_date,end_date):\n",
    "    \n",
    "    num = int((end_date - start_date).days/366) + 1\n",
    "\n",
    "    if num == 1:\n",
    "        dates_l = [[start_date,end_date]]\n",
    "    else:\n",
    "        \n",
    "        dates_l = []\n",
    "        dates_l.append([start_date,start_date + timedelta(days = 365)])\n",
    "        for x in range(0,num-2):\n",
    "            dates_l.append([dates_l[-1][1] + timedelta(days = 1),dates_l[-1][1] + timedelta(days = 366)])\n",
    "        dates_l.append([dates_l[-1][1] + timedelta(days = 1),end_date])\n",
    "        \n",
    "    return dates_l\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temps(pref_no,start_date, end_date):\n",
    "     \n",
    "    # Get pref name and list of station names\n",
    "    pref_name = get_prefs(0)[pref_no]\n",
    "    station_names = get_station(pref_no)\n",
    "    flag = 0\n",
    "\n",
    "    for sn in tqdm(station_names):\n",
    "        try:\n",
    "            element = get_elements(get_aggrgPeriods()[\"時別値\"][0])[\"気温\"][0]\n",
    "            station = get_station(pref_no)[sn][\"id\"]\n",
    "            phpsessid = get_phpsessid()\n",
    "            csv_file = download_hourly_csv(phpsessid, station, element,\n",
    "                            start_date,end_date)\n",
    "            # Delete the Japanese headers\n",
    "            csv_file = csv_file.split(\"\\n\",5)[5]\n",
    "            csv_file = StringIO(csv_file)\n",
    "            col_names = ['Date_Time',sn,'x1','x2']\n",
    "            pdv = pd.read_csv(csv_file, sep=',', header= None, names = col_names).set_index('Date_Time').drop(['x1','x2'], axis=1)\n",
    "\n",
    "            if flag == 0:\n",
    "                pdvf = pdv\n",
    "                flag = 1\n",
    "            else:\n",
    "                pdvf = pdvf.merge(pdv,how = 'outer', on= 'Date_Time')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pdvf.dropna(axis = 1, how = 'all')\n",
    "    \n",
    "    return pdvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "co1 = [51,\n",
    " 52,\n",
    " 53,\n",
    " 54,\n",
    " 55,\n",
    " 56,\n",
    " 57]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[51, 52, 53, 54, 55, 56, 57]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 愛知 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [02:26<00:00,  5.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  愛知 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [02:07<00:00,  4.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  愛知 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [01:58<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  愛知 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [02:17<00:00,  4.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  愛知 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [01:58<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  愛知 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 24/24 [01:05<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 岐阜 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [03:14<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  岐阜 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [02:57<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  岐阜 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [02:42<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  岐阜 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [03:07<00:00,  4.25s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  岐阜 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [03:06<00:00,  4.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  岐阜 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 48/48 [01:28<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 三重 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 27/27 [35:05<00:00, 105.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  三重 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [02:15<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  三重 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [01:42<00:00,  1.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  三重 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [01:28<00:00,  4.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  三重 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [02:02<00:00,  3.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  三重 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 27/27 [01:02<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 新潟 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [04:28<00:00,  4.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  新潟 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [03:57<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  新潟 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [03:43<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  新潟 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [03:58<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  新潟 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [04:11<00:00,  4.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  新潟 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 56/56 [01:58<00:00,  2.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 富山 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [01:35<00:00,  4.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  富山 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [01:26<00:00,  3.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  富山 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [01:16<00:00,  3.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  富山 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [01:19<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  富山 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [01:30<00:00,  4.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  富山 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [00:41<00:00,  1.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 石川 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:31<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  石川 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:27<00:00,  3.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  石川 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:14<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  石川 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:21<00:00,  3.67s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  石川 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [01:29<00:00,  4.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  石川 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:38<00:00,  1.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 / 6 Downloading temps for 福井 2014-01-01 2015-01-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [01:27<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 / 6 Downloading temps for  福井 2015-01-02 2016-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [01:26<00:00,  4.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 / 6 Downloading temps for  福井 2016-01-03 2017-01-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [01:17<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 / 6 Downloading temps for  福井 2017-01-03 2018-01-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [01:23<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 / 6 Downloading temps for  福井 2018-01-04 2019-01-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [01:28<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 / 6 Downloading temps for  福井 2019-01-05 2019-06-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 18/18 [00:40<00:00,  2.26s/it]\n"
     ]
    }
   ],
   "source": [
    "for prefs in co1:\n",
    "\n",
    "    pref_no = prefs\n",
    "    pref_name = get_prefs(0)[pref_no]\n",
    "    start_date = date(2014, 1, 1)\n",
    "    end_date = date(2019, 6, 1)\n",
    "    flag = 0\n",
    "    my_dates = get_dates(start_date, end_date)\n",
    "    no_dates = len(my_dates)\n",
    "\n",
    "    for d in my_dates:\n",
    "        if flag == 0:\n",
    "            print(flag + 1,'/',no_dates,'Downloading temps for',pref_name, d[0],d[1])\n",
    "            data_df = get_temps(pref_no,d[0], d[1])\n",
    "            flag = flag + 1\n",
    "        else:\n",
    "            print(flag + 1,'/',no_dates,'Downloading temps for ',pref_name,d[0],d[1])\n",
    "            data_df = pd.concat([data_df,get_temps(pref_no,d[0], d[1])], sort = True)\n",
    "            flag = flag + 1\n",
    "\n",
    "    data_df = data_df.dropna(axis = 1, how = 'all')   \n",
    "    path = r\"C:\\Users\\phil.richards\\Documents\\Temp_Data\"\n",
    "    filename = f'{pref_name}{d[1]}'\n",
    "    dest = os.path.join(path,filename)\n",
    "    data_df.to_csv(dest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 91,\n",
       " 99]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = list(get_prefs(0).keys())\n",
    "a.sort()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{11: '宗谷',\n",
       " 13: '留萌',\n",
       " 12: '上川',\n",
       " 17: '網走・北見・紋別',\n",
       " 15: '空知',\n",
       " 16: '後志',\n",
       " 14: '石狩',\n",
       " 22: '日高',\n",
       " 20: '十勝',\n",
       " 18: '根室',\n",
       " 24: '檜山',\n",
       " 21: '胆振',\n",
       " 19: '釧路',\n",
       " 23: '渡島',\n",
       " 31: '青森',\n",
       " 32: '秋田',\n",
       " 33: '岩手',\n",
       " 56: '石川',\n",
       " 35: '山形',\n",
       " 34: '宮城',\n",
       " 55: '富山',\n",
       " 54: '新潟',\n",
       " 36: '福島',\n",
       " 81: '山口',\n",
       " 68: '島根',\n",
       " 69: '鳥取',\n",
       " 63: '兵庫',\n",
       " 61: '京都',\n",
       " 57: '福井',\n",
       " 52: '岐阜',\n",
       " 48: '長野',\n",
       " 42: '群馬',\n",
       " 41: '栃木',\n",
       " 40: '茨城',\n",
       " 67: '広島',\n",
       " 66: '岡山',\n",
       " 60: '滋賀',\n",
       " 49: '山梨',\n",
       " 43: '埼玉',\n",
       " 84: '長崎',\n",
       " 85: '佐賀',\n",
       " 82: '福岡',\n",
       " 62: '大阪',\n",
       " 64: '奈良',\n",
       " 53: '三重',\n",
       " 51: '愛知',\n",
       " 50: '静岡',\n",
       " 46: '神奈川',\n",
       " 44: '東京',\n",
       " 45: '千葉',\n",
       " 86: '熊本',\n",
       " 83: '大分',\n",
       " 73: '愛媛',\n",
       " 72: '香川',\n",
       " 65: '和歌山',\n",
       " 88: '鹿児島',\n",
       " 87: '宮崎',\n",
       " 74: '高知',\n",
       " 71: '徳島',\n",
       " 91: '沖縄',\n",
       " 99: '南極'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_prefs(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
