{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "気象庁から過去の気象データを CSV 形式でダウンロードする。\n",
    "API が提供されていないので、ウェブページを参考にスクリプトを作成した。\n",
    "\n",
    "http://www.data.jma.go.jp/gmd/risk/obsdl/index.php\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from datetime import date, timedelta\n",
    "import urllib.request\n",
    "import lxml.html\n",
    "import pandas as pd\n",
    "import codecs as cd\n",
    "import csv\n",
    "from io import StringIO\n",
    "from tqdm import tqdm\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(data):\n",
    "    return urllib.parse.urlencode(data).encode(encoding='ascii')\n",
    "\n",
    "def get_phpsessid():\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/index.php\"\n",
    "    xml = urllib.request.urlopen(URL).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "    return tree.cssselect(\"input#sid\")[0].value\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns obsevation stations in prefectures\n",
    "\n",
    "def get_station(pd=0):\n",
    "    assert type(pd) is int and pd > 0\n",
    "    \n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/station\"\n",
    "    data = encode_data({\"pd\": \"%02d\" % pd})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def kansoku_items(bits):\n",
    "        return dict(rain=(bits[0] == \"1\"),\n",
    "                    wind=(bits[1] == \"1\"),\n",
    "                    temp=(bits[2] == \"1\"),\n",
    "                    sun =(bits[3] == \"1\"),\n",
    "                    snow=(bits[4] == \"1\"))\n",
    "\n",
    "    def parse_station(dom):\n",
    "        stitle = dom.get(\"title\").replace(\"：\", \":\")\n",
    "        title = dict(filter(lambda y: len(y) == 2,\n",
    "                            map(lambda x: x.split(\":\"), stitle.split(\"\\n\"))))\n",
    "                                \n",
    "        name    = title[\"地点名\"]\n",
    "        stid    = dom.cssselect(\"input[name=stid]\")[0].value\n",
    "        stname  = dom.cssselect(\"input[name=stname]\")[0].value\n",
    "        kansoku = kansoku_items(dom.cssselect(\"input[name=kansoku]\")[0].value)\n",
    "        assert name == stname\n",
    "        return (stname, dict(id=stid, flags=kansoku))\n",
    "    \n",
    "    \n",
    "    stations = dict(map(parse_station, tree.cssselect(\"div.station\")))\n",
    "    \n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precture Dictionary\n",
    "# order 0: id -> name, 1: name -> id\n",
    "\n",
    "def get_prefs(order, pd=0):\n",
    "        \n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/station\"\n",
    "    data = encode_data({\"pd\": \"%02d\" % pd})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def parse_prefs(dom):\n",
    "        name = dom.text\n",
    "        prid = int(dom.cssselect(\"input[name=prid]\")[0].value)\n",
    "        if order == 0:\n",
    "            return (prid, name)\n",
    "        else:\n",
    "            return (name, prid)\n",
    "    \n",
    "    stations = dict(map(parse_prefs, tree.cssselect(\"div.prefecture\")))\n",
    "        \n",
    "    return stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_aggrgPeriods():\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/element\"\n",
    "    xml = urllib.request.urlopen(URL).read().decode(\"utf-8\")  # HTTP GET\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    def parse_periods(dom):\n",
    "        if dom.find(\"label\") is not None:\n",
    "            val = dom.find(\"label/input\").attrib[\"value\"]\n",
    "            key = dom.find(\"label/span\").text\n",
    "            rng = None\n",
    "        else:\n",
    "            val = dom.find(\"input\").attrib[\"value\"]\n",
    "            key = dom.find(\"span/label\").text\n",
    "            rng = list(map(lambda x: int(x.get(\"value\")),\n",
    "                           dom.find(\"span/select\").getchildren()))\n",
    "        return (key, (val, rng))\n",
    "\n",
    "    perdoms = tree.cssselect(\"#aggrgPeriod\")[0].find(\"div/div\").getchildren()\n",
    "    periods = dict(map(parse_periods, perdoms))\n",
    "    return periods\n",
    "\n",
    "def get_elements(aggrgPeriods=9, isTypeNumber=1):\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/top/element\"\n",
    "    data = encode_data({\"aggrgPeriod\": aggrgPeriods,\n",
    "                        \"isTypeNumber\": isTypeNumber})\n",
    "    xml = urllib.request.urlopen(URL, data=data).read().decode(\"utf-8\")\n",
    "    open(\"tmp.html\", \"w\").write(xml)\n",
    "    tree = lxml.html.fromstring(xml)\n",
    "\n",
    "    boxes = tree.cssselect(\"input[type=checkbox]\")\n",
    "    options, items = boxes[0:4], boxes[4:]\n",
    "\n",
    "    def parse_items(dom):\n",
    "        if \"disabled\" in dom.attrib: return None\n",
    "        if dom.name == \"kijiFlag\": return None\n",
    "        name     = dom.attrib[\"id\"]\n",
    "        value    = dom.attrib[\"value\"]\n",
    "        options  = None\n",
    "        select = dom.getnext().find(\"select\")\n",
    "        if select is not None:\n",
    "            options = list(map(lambda x: int(x.get(\"value\")),\n",
    "                               select.getchildren()))\n",
    "        return (name, (value, options))\n",
    "    \n",
    "    items = dict(filter(lambda x: x, map(parse_items, items)))\n",
    "    return items\n",
    "\n",
    "\n",
    "def download_hourly_csv(phpsessid, station, element, begin_date, end_date):\n",
    "    params = {\n",
    "        \"PHPSESSID\": phpsessid,\n",
    "        # 共通フラグ\n",
    "        \"rmkFlag\": 1,        # 利用上注意が必要なデータを格納する\n",
    "        \"disconnectFlag\": 1, # 観測環境の変化にかかわらずデータを格納する\n",
    "        \"csvFlag\": 1,        # すべて数値で格納する\n",
    "        \"ymdLiteral\": 1,     # 日付は日付リテラルで格納する\n",
    "        \"youbiFlag\": 0,      # 日付に曜日を表示する\n",
    "        \"kijiFlag\": 0,       # 最高・最低（最大・最小）値の発生時刻を表示\n",
    "        # 時別値データ選択\n",
    "        \"aggrgPeriod\": 9,    # 時別値\n",
    "        \"stationNumList\": '[\"%s\"]' % station,      # 観測地点IDのリスト\n",
    "        \"elementNumList\": '[[\"%s\",\"\"]]' % element, # 項目IDのリスト\n",
    "        \"ymdList\": '[\"%d\", \"%d\", \"%d\", \"%d\", \"%d\", \"%d\"]' % (\n",
    "            begin_date.year,  end_date.year,\n",
    "            begin_date.month, end_date.month,\n",
    "            begin_date.day,   end_date.day),       # 取得する期間\n",
    "        \"jikantaiFlag\": 0,        # 特定の時間帯のみ表示する\n",
    "        \"jikantaiList\": '[1,24]', # デフォルトは全部\n",
    "        \"interAnnualFlag\": 1,     # 連続した期間で表示する\n",
    "        # 以下、意味の分からないフラグ類\n",
    "        \"optionNumList\": [],\n",
    "        \"downloadFlag\": \"true\",   # CSV としてダウンロードする？\n",
    "        \"huukouFlag\": 0,\n",
    "    }\n",
    "\n",
    "    URL=\"http://www.data.jma.go.jp/gmd/risk/obsdl/show/table\"\n",
    "    data = encode_data(params)\n",
    "    csv_data = urllib.request.urlopen(URL, data=data).read().decode(\"shift-jis\")\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return list of start and end-dates\n",
    "# As max size of data is about 1 years worth of hourly data it divides period into yearly segments\n",
    "\n",
    "def get_dates(start_date,end_date):\n",
    "    \n",
    "    num = int((end_date - start_date).days/366) + 1\n",
    "\n",
    "    if num == 1:\n",
    "        dates_l = [[start_date,end_date]]\n",
    "    else:\n",
    "        \n",
    "        dates_l = []\n",
    "        dates_l.append([start_date,start_date + timedelta(days = 365)])\n",
    "        for x in range(0,num-2):\n",
    "            dates_l.append([dates_l[-1][1] + timedelta(days = 1),dates_l[-1][1] + timedelta(days = 366)])\n",
    "        dates_l.append([dates_l[-1][1] + timedelta(days = 1),end_date])\n",
    "        \n",
    "    return dates_l\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_temps(pref_no,start_date, end_date):\n",
    "     \n",
    "    # Get pref name and list of station names\n",
    "    pref_name = get_prefs(0)[pref_no]\n",
    "    station_names = get_station(pref_no)\n",
    "    flag = 0\n",
    "\n",
    "    for sn in tqdm(station_names):\n",
    "        try:\n",
    "            element = get_elements(get_aggrgPeriods()[\"時別値\"][0])[\"気温\"][0]\n",
    "            station = get_station(pref_no)[sn][\"id\"]\n",
    "            phpsessid = get_phpsessid()\n",
    "            csv_file = download_hourly_csv(phpsessid, station, element,\n",
    "                            start_date,end_date)\n",
    "            # Delete the Japanese headers\n",
    "            csv_file = csv_file.split(\"\\n\",5)[5]\n",
    "            csv_file = StringIO(csv_file)\n",
    "            col_names = ['Date_Time',sn,'x1','x2']\n",
    "            pdv = pd.read_csv(csv_file, sep=',', header= None, names = col_names).set_index('Date_Time').drop(['x1','x2'], axis=1)\n",
    "\n",
    "            if flag == 0:\n",
    "                pdvf = pdv\n",
    "                flag = 1\n",
    "            else:\n",
    "                pdvf = pdvf.merge(pdv,how = 'outer', on= 'Date_Time')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    pdvf.dropna(axis = 1, how = 'all')\n",
    "    \n",
    "    return pdvf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the numbers of prefectures to download\n",
    "# use get_prefs(0) for prefecture codes\n",
    "\n",
    "co1 = [13,\n",
    " 14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prefs in co1:\n",
    "\n",
    "    pref_no = prefs\n",
    "    pref_name = get_prefs(0)[pref_no]\n",
    "    start_date = date(2014, 1, 1)\n",
    "    end_date = date(2019, 6, 1)\n",
    "    flag = 0\n",
    "    my_dates = get_dates(start_date, end_date)\n",
    "    no_dates = len(my_dates)\n",
    "\n",
    "    for d in my_dates:\n",
    "        if flag == 0:\n",
    "            print(flag + 1,'/',no_dates,'Downloading temps for',pref_name, d[0],d[1])\n",
    "            data_df = get_temps(pref_no,d[0], d[1])\n",
    "            flag = flag + 1\n",
    "        else:\n",
    "            print(flag + 1,'/',no_dates,'Downloading temps for ',pref_name,d[0],d[1])\n",
    "            data_df = pd.concat([data_df,get_temps(pref_no,d[0], d[1])], sort = True)\n",
    "            flag = flag + 1\n",
    "\n",
    "    data_df = data_df.dropna(axis = 1, how = 'all')   \n",
    "    path = r\"C:\\Users\\phil.richards\\Documents\\Temp_Data\"\n",
    "    filename = f'{pref_no}_{pref_name}_{d[1]}'\n",
    "    dest = os.path.join(path,filename)\n",
    "    data_df.to_csv(dest)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
